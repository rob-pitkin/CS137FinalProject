{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# CS137 Final Project - HuBMAP - Hacking the Human Body"
      ],
      "metadata": {
        "id": "JE_F8p_vJfC2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports and environment setup"
      ],
      "metadata": {
        "id": "2LBDIKOrJmF9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# If use google colab, mount the working directory there. \n",
        "from google.colab import drive\n",
        "import sys\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# NOTE: you need to use your own path to add the implementation to the python path \n",
        "# so you can import functions from implementation.py\n",
        "sys.path.append('/content/drive/MyDrive/CS137_Final_Project')"
      ],
      "metadata": {
        "id": "qAj_nR7oJbNj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "812fdc53-b7dc-4763-b425-be94b7a13677"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# A bit of setup\n",
        "import numpy as np\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import sys\n",
        "\n",
        "%matplotlib inline\n",
        "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
        "plt.rcParams['image.interpolation'] = 'nearest'\n",
        "plt.rcParams['image.cmap'] = 'gray'\n",
        "\n",
        "# for auto-reloading external modules\n",
        "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "2nICHUd8JP73"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing UNet Model functions"
      ],
      "metadata": {
        "id": "zdEkIluvZuRM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from unet_utils import *\n",
        "from SimpleUNet import *"
      ],
      "metadata": {
        "id": "e2P-K7NfNC7b"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Instantiating UNet and checking model summary"
      ],
      "metadata": {
        "id": "htjPyfRRNnMQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchsummary import summary\n",
        "model = UNetModel()\n",
        "model.to(device)\n",
        "summary(model, input_size=(1, 512, 512))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uSf3ulHINPQL",
        "outputId": "fe2663ad-1c6a-424f-ebd6-2dc2b9241c67"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/lazy.py:178: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
            "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1         [-1, 64, 512, 512]             640\n",
            "       BatchNorm2d-2         [-1, 64, 512, 512]             128\n",
            "              ReLU-3         [-1, 64, 512, 512]               0\n",
            "            Conv2d-4         [-1, 64, 512, 512]          36,928\n",
            "       BatchNorm2d-5         [-1, 64, 512, 512]             128\n",
            "              ReLU-6         [-1, 64, 512, 512]               0\n",
            "         ConvBlock-7         [-1, 64, 512, 512]               0\n",
            "         MaxPool2d-8         [-1, 64, 256, 256]               0\n",
            "            Conv2d-9        [-1, 128, 256, 256]          73,856\n",
            "      BatchNorm2d-10        [-1, 128, 256, 256]             256\n",
            "             ReLU-11        [-1, 128, 256, 256]               0\n",
            "           Conv2d-12        [-1, 128, 256, 256]         147,584\n",
            "      BatchNorm2d-13        [-1, 128, 256, 256]             256\n",
            "             ReLU-14        [-1, 128, 256, 256]               0\n",
            "        ConvBlock-15        [-1, 128, 256, 256]               0\n",
            "        DownBlock-16        [-1, 128, 256, 256]               0\n",
            "        MaxPool2d-17        [-1, 128, 128, 128]               0\n",
            "           Conv2d-18        [-1, 256, 128, 128]         295,168\n",
            "      BatchNorm2d-19        [-1, 256, 128, 128]             512\n",
            "             ReLU-20        [-1, 256, 128, 128]               0\n",
            "           Conv2d-21        [-1, 256, 128, 128]         590,080\n",
            "      BatchNorm2d-22        [-1, 256, 128, 128]             512\n",
            "             ReLU-23        [-1, 256, 128, 128]               0\n",
            "        ConvBlock-24        [-1, 256, 128, 128]               0\n",
            "        DownBlock-25        [-1, 256, 128, 128]               0\n",
            "        MaxPool2d-26          [-1, 256, 64, 64]               0\n",
            "           Conv2d-27          [-1, 512, 64, 64]       1,180,160\n",
            "      BatchNorm2d-28          [-1, 512, 64, 64]           1,024\n",
            "             ReLU-29          [-1, 512, 64, 64]               0\n",
            "           Conv2d-30          [-1, 512, 64, 64]       2,359,808\n",
            "      BatchNorm2d-31          [-1, 512, 64, 64]           1,024\n",
            "             ReLU-32          [-1, 512, 64, 64]               0\n",
            "        ConvBlock-33          [-1, 512, 64, 64]               0\n",
            "        DownBlock-34          [-1, 512, 64, 64]               0\n",
            "        MaxPool2d-35          [-1, 512, 32, 32]               0\n",
            "           Conv2d-36         [-1, 1024, 32, 32]       4,719,616\n",
            "      BatchNorm2d-37         [-1, 1024, 32, 32]           2,048\n",
            "             ReLU-38         [-1, 1024, 32, 32]               0\n",
            "           Conv2d-39         [-1, 1024, 32, 32]       9,438,208\n",
            "      BatchNorm2d-40         [-1, 1024, 32, 32]           2,048\n",
            "             ReLU-41         [-1, 1024, 32, 32]               0\n",
            "        ConvBlock-42         [-1, 1024, 32, 32]               0\n",
            "        DownBlock-43         [-1, 1024, 32, 32]               0\n",
            "  ConvTranspose2d-44          [-1, 512, 64, 64]       2,097,664\n",
            "           Conv2d-45          [-1, 512, 64, 64]       4,719,104\n",
            "      BatchNorm2d-46          [-1, 512, 64, 64]           1,024\n",
            "             ReLU-47          [-1, 512, 64, 64]               0\n",
            "           Conv2d-48          [-1, 512, 64, 64]       2,359,808\n",
            "      BatchNorm2d-49          [-1, 512, 64, 64]           1,024\n",
            "             ReLU-50          [-1, 512, 64, 64]               0\n",
            "        ConvBlock-51          [-1, 512, 64, 64]               0\n",
            "          UpBlock-52          [-1, 512, 64, 64]               0\n",
            "  ConvTranspose2d-53        [-1, 256, 128, 128]         524,544\n",
            "           Conv2d-54        [-1, 256, 128, 128]       1,179,904\n",
            "      BatchNorm2d-55        [-1, 256, 128, 128]             512\n",
            "             ReLU-56        [-1, 256, 128, 128]               0\n",
            "           Conv2d-57        [-1, 256, 128, 128]         590,080\n",
            "      BatchNorm2d-58        [-1, 256, 128, 128]             512\n",
            "             ReLU-59        [-1, 256, 128, 128]               0\n",
            "        ConvBlock-60        [-1, 256, 128, 128]               0\n",
            "          UpBlock-61        [-1, 256, 128, 128]               0\n",
            "  ConvTranspose2d-62        [-1, 128, 256, 256]         131,200\n",
            "           Conv2d-63        [-1, 128, 256, 256]         295,040\n",
            "      BatchNorm2d-64        [-1, 128, 256, 256]             256\n",
            "             ReLU-65        [-1, 128, 256, 256]               0\n",
            "           Conv2d-66        [-1, 128, 256, 256]         147,584\n",
            "      BatchNorm2d-67        [-1, 128, 256, 256]             256\n",
            "             ReLU-68        [-1, 128, 256, 256]               0\n",
            "        ConvBlock-69        [-1, 128, 256, 256]               0\n",
            "          UpBlock-70        [-1, 128, 256, 256]               0\n",
            "  ConvTranspose2d-71         [-1, 64, 512, 512]          32,832\n",
            "           Conv2d-72         [-1, 64, 512, 512]          73,792\n",
            "      BatchNorm2d-73         [-1, 64, 512, 512]             128\n",
            "             ReLU-74         [-1, 64, 512, 512]               0\n",
            "           Conv2d-75         [-1, 64, 512, 512]          36,928\n",
            "      BatchNorm2d-76         [-1, 64, 512, 512]             128\n",
            "             ReLU-77         [-1, 64, 512, 512]               0\n",
            "        ConvBlock-78         [-1, 64, 512, 512]               0\n",
            "          UpBlock-79         [-1, 64, 512, 512]               0\n",
            "           Conv2d-80          [-1, 5, 512, 512]             325\n",
            "         OutBlock-81          [-1, 5, 512, 512]               0\n",
            "================================================================\n",
            "Total params: 31,042,629\n",
            "Trainable params: 31,042,629\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 1.00\n",
            "Forward/backward pass size (MB): 4096.00\n",
            "Params size (MB): 118.42\n",
            "Estimated Total Size (MB): 4215.42\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training the Model"
      ],
      "metadata": {
        "id": "96e5DWG3MbKZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "from loss_functions import *\n",
        "from torchvision.transforms import ToTensor, Compose\n",
        "from torch.utils.data import random_split, DataLoader\n",
        "\n",
        "\n",
        "def train():\n",
        "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "    print(device)\n",
        "\n",
        "    # from bean_dataset import BeanImageDataset\n",
        "    #\n",
        "    # trainset = BeanImageDataset(\"/content/drive/MyDrive/CS137_Assignment3_RobPitkin/data/train\")\n",
        "    # validset = BeanImageDataset(\"/content/drive/MyDrive/CS137_Assignment3_RobPitkin/data/validation\")\n",
        "\n",
        "    train_loader = DataLoader(trainset, batch_size=64, shuffle=True)\n",
        "    valid_loader = DataLoader(validset, batch_size=64, shuffle=True)\n",
        "\n",
        "    if device.type == \"cuda\":\n",
        "        total_mem = torch.cuda.get_device_properties(0).total_memory\n",
        "    else:\n",
        "        total_mem = 0\n",
        "\n",
        "    epochs = 100\n",
        "    learning_rate = 0.00001\n",
        "\n",
        "    model = UNetModel()\n",
        "    model.to(device)\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    criterion = DiceLoss(mode='multiclass', from_logits=True)\n",
        "\n",
        "    # Recording the loss\n",
        "    train_loss = []\n",
        "    val_loss = []\n",
        "    val_acc = []\n",
        "\n",
        "    for i in range(epochs):\n",
        "        running_train_loss = 0.0\n",
        "        for j, data in enumerate(train_loader):\n",
        "            x, y = data\n",
        "            ## If GPU is available, move to cuda\n",
        "            if device.type == \"cuda\":\n",
        "                x = x.to(device)\n",
        "                y = y.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            output = model(x)\n",
        "            loss = criterion(output, y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            if device.type == \"cuda\":\n",
        "                loss = loss.cpu()\n",
        "\n",
        "            running_train_loss += np.mean(loss.data.numpy())\n",
        "\n",
        "        running_train_loss /= train_loader.__len__()\n",
        "        train_loss.append(running_train_loss)\n",
        "\n",
        "        # validate\n",
        "        running_val_loss = 0.0\n",
        "        running_val_acc = 0.0\n",
        "        with torch.no_grad():\n",
        "            for k, data in enumerate(valid_loader):\n",
        "                x, y = data\n",
        "                if device.type == \"cuda\":\n",
        "                    x = x.to(device)\n",
        "                    y = y.to(device)\n",
        "                output = model(x)\n",
        "                loss = criterion(output, y)\n",
        "\n",
        "                if device.type == \"cuda\":\n",
        "                    loss = loss.cpu()\n",
        "\n",
        "                running_val_loss += np.mean(loss.data.numpy())\n",
        "\n",
        "                for l in range(len(x)):\n",
        "                    pred = torch.nn.functional.softmax(output[l], dim=0)\n",
        "                    if torch.argmax(pred) == y[l]:\n",
        "                        running_val_acc += 1\n",
        "\n",
        "            running_val_acc /= len(valid_loader.dataset.y)\n",
        "            running_val_loss /= valid_loader.__len__()\n",
        "            if (len(val_loss) != 0 and np.mean(running_val_loss) < min(val_loss)):\n",
        "                torch.save(model.state_dict(), 'best.sav')\n",
        "            val_loss.append(running_val_loss)\n",
        "            val_acc.append(running_val_acc)\n",
        "\n",
        "        # check GPU memory if necessary\n",
        "        if device.type == \"cuda\":\n",
        "            alloc_mem = torch.cuda.memory_allocated(0)\n",
        "        else:\n",
        "            alloc_mem = 0\n",
        "\n",
        "        # print out\n",
        "        print(\n",
        "            f\"Epoch [{i + 1}]: Training Loss: {running_train_loss} Validation Loss: {running_val_loss} Accuracy: {running_val_acc}\" + (\n",
        "                f\" Allocated/Total GPU memory: {alloc_mem}/{total_mem}\" if device.type == \"cuda\" else \"\"\n",
        "            ))\n"
      ],
      "metadata": {
        "id": "VxeVkjAsf-PF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Plot loss and accuracy curves"
      ],
      "metadata": {
        "id": "QgrSiI52N-oz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.subplot(2, 1, 1)\n",
        "plt.plot(train_loss, '-o')\n",
        "plt.plot(val_loss, '-o')\n",
        "plt.legend(['train', 'val'], loc='upper left')\n",
        "plt.xlabel('iteration')\n",
        "plt.ylabel('loss')\n",
        "\n",
        "plt.subplot(2, 1, 2)\n",
        "plt.plot(val_acc, '-o')\n",
        "\n",
        "plt.legend(['train', 'val'], loc='upper left')\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('accuracy')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "zgFK_Kq2OBav"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save the current model (optional)"
      ],
      "metadata": {
        "id": "dT7iORAYOLe5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model, \"UNet.sav\")"
      ],
      "metadata": {
        "id": "J9ajtTfDOOVL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing the Model"
      ],
      "metadata": {
        "id": "b3ReqYARN1vQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the previously saved model.\n",
        "# testset = BeanImageDataset(\"data/test\")\n",
        "# test_loader = DataLoader(testset, batch_size=1, shuffle=True)\n",
        "\n",
        "# Load the saved model\n",
        "# model = torch.load(\"UNet.sav\")\n",
        "\n",
        "# Load the best model\n",
        "model.load_state_dict(torch.load('best'))\n",
        "model.eval()\n",
        "\n",
        "\n",
        "t_acc = []\n",
        "with torch.no_grad():\n",
        "    # only one item in the iterator\n",
        "    # Add more batches if your device couldn't handle the computation \n",
        "    for _, data in enumerate(test_loader):\n",
        "        x, y = data\n",
        "        x = x.to(device)\n",
        "        y = y.to(device)\n",
        "        y_hat = model(x)\n",
        "\n",
        "        if device.type == \"cuda\":\n",
        "          x = x.to(\"cpu\")\n",
        "          y = y.to(\"cpu\")\n",
        "          y_hat = y_hat.to(\"cpu\")\n",
        "        acc = np.average(y.numpy() == np.argmax(y_hat.numpy(), axis = 1))\n",
        "        t_acc.append(acc.item())\n",
        "print(f\"Test accuracy: {np.average(t_acc)}\")"
      ],
      "metadata": {
        "id": "-5LLDRBTN3SU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
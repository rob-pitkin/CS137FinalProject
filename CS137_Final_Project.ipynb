{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU",
  "gpuClass": "standard"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# CS137 Final Project - HuBMAP - Hacking the Human Body"
   ],
   "metadata": {
    "id": "JE_F8p_vJfC2",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Imports and environment setup"
   ],
   "metadata": {
    "id": "2LBDIKOrJmF9",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# If use google colab, mount the working directory there. \n",
    "from google.colab import drive\n",
    "import sys\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# NOTE: you need to use your own path to add the implementation to the python path \n",
    "# so you can import functions from implementation.py\n",
    "sys.path.append('/content/drive/MyDrive/CS137_Assignment1_RobPitkin')"
   ],
   "metadata": {
    "id": "qAj_nR7oJbNj",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# A bit of setup\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2nICHUd8JP73",
    "outputId": "1965222c-fded-4a83-bad6-dca012c244cc",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 8,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## UNet Helper Implementation"
   ],
   "metadata": {
    "id": "HfSRV_0AJpfw",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Code derived from https://github.com/milesial/Pytorch-UNet/blob/master/unet/unet_parts.py"
   ],
   "metadata": {
    "id": "zdEkIluvZuRM",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "CLOy1KouIvQU",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class ConvBlock(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Convolutional block\n",
    "    \"\"\"\n",
    "    def __init__(self, num_filters=64, dropout_p=0):\n",
    "        super().__init__()\n",
    "\n",
    "        if dropout_p == 0:\n",
    "            self.conv_block = torch.nn.Sequential(\n",
    "                torch.nn.LazyConv2d(num_filters, kernel_size=3, padding='same'),\n",
    "                torch.nn.BatchNorm2d(num_filters),\n",
    "                torch.nn.ReLU(),\n",
    "                torch.nn.LazyConv2d(num_filters, kernel_size=3, padding='same'),\n",
    "                torch.nn.BatchNorm2d(num_filters),\n",
    "                torch.nn.ReLU()\n",
    "            )\n",
    "        else:\n",
    "            self.conv_block = torch.nn.Sequential(\n",
    "                torch.nn.LazyConv2d(num_filters, kernel_size=3, padding='same'),\n",
    "                torch.nn.BatchNorm2d(num_filters),\n",
    "                torch.nn.ReLU(),\n",
    "                torch.nn.LazyConv2d(num_filters, kernel_size=3, padding='same'),\n",
    "                torch.nn.BatchNorm2d(num_filters),\n",
    "                torch.nn.ReLU(),\n",
    "                torch.nn.Dropout(dropout_p)\n",
    "            )\n",
    "    \n",
    "    def forward(self, input):\n",
    "        return self.conv_block(input)"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "class DownBlock(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Downsampling Block\n",
    "    \"\"\"\n",
    "    def __init__(self, out_channels, dropout_p=0):\n",
    "        super().__init__()\n",
    "\n",
    "        self.down_block = torch.nn.Sequential(\n",
    "            torch.nn.MaxPool2d(kernel_size=2),\n",
    "            ConvBlock(out_channels, dropout_p),\n",
    "        )\n",
    "    \n",
    "    def forward(self, input):\n",
    "        return self.down_block(input)"
   ],
   "metadata": {
    "id": "DLyig0tPMhgm",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "class UpBlock(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Upsampling Block\n",
    "    \"\"\"\n",
    "    def __init__(self, out_channels, dropout_p=0):\n",
    "        super().__init__()\n",
    "\n",
    "        self.up = torch.nn.LazyConvTranspose2d(out_channels, kernel_size=2, stride=2)\n",
    "        self.conv_block = ConvBlock(out_channels, dropout_p=dropout_p)\n",
    "    \n",
    "    def forward(self, input, skip):\n",
    "        \"\"\"\n",
    "        Using code derived from https://github.com/milesial/Pytorch-UNet/blob/master/unet/unet_parts.py\n",
    "        \"\"\"\n",
    "        input = self.up(input)\n",
    "        # input is CHW\n",
    "        diffY = skip.size()[2] - input.size()[2]\n",
    "        diffX = skip.size()[3] - input.size()[3]\n",
    "\n",
    "        input = torch.nn.functional.pad(input, [diffX // 2, diffX - diffX // 2,\n",
    "                                                diffY // 2, diffY - diffY // 2])\n",
    "       \n",
    "        x = torch.cat([skip, input], dim=1)\n",
    "        return self.conv_block(x)"
   ],
   "metadata": {
    "id": "vzVGuk5KNnVR",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "class OutBlock(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Final output block\n",
    "    \"\"\"\n",
    "    def __init__(self, out_channels):\n",
    "        super(OutBlock, self).__init__()\n",
    "\n",
    "        self.conv_layer = torch.nn.LazyConv2d(out_channels=out_channels, kernel_size=1, padding='same')\n",
    "    \n",
    "    def forward(self, input):\n",
    "        return self.conv_layer(input)\n"
   ],
   "metadata": {
    "id": "-13IHiHPayEF",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Implementing the UNet Model"
   ],
   "metadata": {
    "id": "-xetiYLgcK7_",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class UNetModel(torch.nn.Module):\n",
    "    def __init__(self, num_classes=5):\n",
    "        super(UNetModel, self).__init__()\n",
    "\n",
    "        self.conv_block = ConvBlock(64)\n",
    "        self.down_block1 = DownBlock(128)\n",
    "        self.down_block2 = DownBlock(256)\n",
    "        self.down_block3 = DownBlock(512)\n",
    "        self.down_block4 = DownBlock(1024)\n",
    "        self.up_block4 = UpBlock(512)\n",
    "        self.up_block3 = UpBlock(256)\n",
    "        self.up_block2 = UpBlock(128)\n",
    "        self.up_block1 = UpBlock(64)\n",
    "        self.out_block = OutBlock(num_classes)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        x1 = self.conv_block(input)\n",
    "        x2 = self.down_block1(x1)\n",
    "        x3 = self.down_block2(x2)\n",
    "        x4 = self.down_block3(x3)\n",
    "        x5 = self.down_block4(x4)\n",
    "        out = self.up_block4(x5, x4)\n",
    "        out = self.up_block3(out, x3)\n",
    "        out = self.up_block2(out, x2)\n",
    "        out = self.up_block1(out, x1)\n",
    "        out = self.out_block(out)\n",
    "        return out"
   ],
   "metadata": {
    "id": "GBcoASBUcKKA",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 10,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Instantiating model and checking summary"
   ],
   "metadata": {
    "id": "dfFaXSAqejAh",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from torchsummary import summary\n",
    "model = UNetModel()\n",
    "model.to(device)\n",
    "summary(model, input_size=(1, 512, 512))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2Vyu863seaNz",
    "outputId": "e2e92e1a-18cb-4c75-8ee6-b317d2200a06",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 11,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 512, 512]             640\n",
      "       BatchNorm2d-2         [-1, 64, 512, 512]             128\n",
      "              ReLU-3         [-1, 64, 512, 512]               0\n",
      "            Conv2d-4         [-1, 64, 512, 512]          36,928\n",
      "       BatchNorm2d-5         [-1, 64, 512, 512]             128\n",
      "              ReLU-6         [-1, 64, 512, 512]               0\n",
      "         ConvBlock-7         [-1, 64, 512, 512]               0\n",
      "         MaxPool2d-8         [-1, 64, 256, 256]               0\n",
      "            Conv2d-9        [-1, 128, 256, 256]          73,856\n",
      "      BatchNorm2d-10        [-1, 128, 256, 256]             256\n",
      "             ReLU-11        [-1, 128, 256, 256]               0\n",
      "           Conv2d-12        [-1, 128, 256, 256]         147,584\n",
      "      BatchNorm2d-13        [-1, 128, 256, 256]             256\n",
      "             ReLU-14        [-1, 128, 256, 256]               0\n",
      "        ConvBlock-15        [-1, 128, 256, 256]               0\n",
      "        DownBlock-16        [-1, 128, 256, 256]               0\n",
      "        MaxPool2d-17        [-1, 128, 128, 128]               0\n",
      "           Conv2d-18        [-1, 256, 128, 128]         295,168\n",
      "      BatchNorm2d-19        [-1, 256, 128, 128]             512\n",
      "             ReLU-20        [-1, 256, 128, 128]               0\n",
      "           Conv2d-21        [-1, 256, 128, 128]         590,080\n",
      "      BatchNorm2d-22        [-1, 256, 128, 128]             512\n",
      "             ReLU-23        [-1, 256, 128, 128]               0\n",
      "        ConvBlock-24        [-1, 256, 128, 128]               0\n",
      "        DownBlock-25        [-1, 256, 128, 128]               0\n",
      "        MaxPool2d-26          [-1, 256, 64, 64]               0\n",
      "           Conv2d-27          [-1, 512, 64, 64]       1,180,160\n",
      "      BatchNorm2d-28          [-1, 512, 64, 64]           1,024\n",
      "             ReLU-29          [-1, 512, 64, 64]               0\n",
      "           Conv2d-30          [-1, 512, 64, 64]       2,359,808\n",
      "      BatchNorm2d-31          [-1, 512, 64, 64]           1,024\n",
      "             ReLU-32          [-1, 512, 64, 64]               0\n",
      "        ConvBlock-33          [-1, 512, 64, 64]               0\n",
      "        DownBlock-34          [-1, 512, 64, 64]               0\n",
      "        MaxPool2d-35          [-1, 512, 32, 32]               0\n",
      "           Conv2d-36         [-1, 1024, 32, 32]       4,719,616\n",
      "      BatchNorm2d-37         [-1, 1024, 32, 32]           2,048\n",
      "             ReLU-38         [-1, 1024, 32, 32]               0\n",
      "           Conv2d-39         [-1, 1024, 32, 32]       9,438,208\n",
      "      BatchNorm2d-40         [-1, 1024, 32, 32]           2,048\n",
      "             ReLU-41         [-1, 1024, 32, 32]               0\n",
      "        ConvBlock-42         [-1, 1024, 32, 32]               0\n",
      "        DownBlock-43         [-1, 1024, 32, 32]               0\n",
      "  ConvTranspose2d-44          [-1, 512, 64, 64]       2,097,664\n",
      "           Conv2d-45          [-1, 512, 64, 64]       4,719,104\n",
      "      BatchNorm2d-46          [-1, 512, 64, 64]           1,024\n",
      "             ReLU-47          [-1, 512, 64, 64]               0\n",
      "           Conv2d-48          [-1, 512, 64, 64]       2,359,808\n",
      "      BatchNorm2d-49          [-1, 512, 64, 64]           1,024\n",
      "             ReLU-50          [-1, 512, 64, 64]               0\n",
      "        ConvBlock-51          [-1, 512, 64, 64]               0\n",
      "          UpBlock-52          [-1, 512, 64, 64]               0\n",
      "  ConvTranspose2d-53        [-1, 256, 128, 128]         524,544\n",
      "           Conv2d-54        [-1, 256, 128, 128]       1,179,904\n",
      "      BatchNorm2d-55        [-1, 256, 128, 128]             512\n",
      "             ReLU-56        [-1, 256, 128, 128]               0\n",
      "           Conv2d-57        [-1, 256, 128, 128]         590,080\n",
      "      BatchNorm2d-58        [-1, 256, 128, 128]             512\n",
      "             ReLU-59        [-1, 256, 128, 128]               0\n",
      "        ConvBlock-60        [-1, 256, 128, 128]               0\n",
      "          UpBlock-61        [-1, 256, 128, 128]               0\n",
      "  ConvTranspose2d-62        [-1, 128, 256, 256]         131,200\n",
      "           Conv2d-63        [-1, 128, 256, 256]         295,040\n",
      "      BatchNorm2d-64        [-1, 128, 256, 256]             256\n",
      "             ReLU-65        [-1, 128, 256, 256]               0\n",
      "           Conv2d-66        [-1, 128, 256, 256]         147,584\n",
      "      BatchNorm2d-67        [-1, 128, 256, 256]             256\n",
      "             ReLU-68        [-1, 128, 256, 256]               0\n",
      "        ConvBlock-69        [-1, 128, 256, 256]               0\n",
      "          UpBlock-70        [-1, 128, 256, 256]               0\n",
      "  ConvTranspose2d-71         [-1, 64, 512, 512]          32,832\n",
      "           Conv2d-72         [-1, 64, 512, 512]          73,792\n",
      "      BatchNorm2d-73         [-1, 64, 512, 512]             128\n",
      "             ReLU-74         [-1, 64, 512, 512]               0\n",
      "           Conv2d-75         [-1, 64, 512, 512]          36,928\n",
      "      BatchNorm2d-76         [-1, 64, 512, 512]             128\n",
      "             ReLU-77         [-1, 64, 512, 512]               0\n",
      "        ConvBlock-78         [-1, 64, 512, 512]               0\n",
      "          UpBlock-79         [-1, 64, 512, 512]               0\n",
      "           Conv2d-80          [-1, 5, 512, 512]             325\n",
      "         OutBlock-81          [-1, 5, 512, 512]               0\n",
      "================================================================\n",
      "Total params: 31,042,629\n",
      "Trainable params: 31,042,629\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 1.00\n",
      "Forward/backward pass size (MB): 4096.00\n",
      "Params size (MB): 118.42\n",
      "Estimated Total Size (MB): 4215.42\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ]
  }
 ]
}
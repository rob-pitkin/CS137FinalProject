{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# CS137 Final Project - HuBMAP - Hacking the Human Body"
      ],
      "metadata": {
        "id": "JE_F8p_vJfC2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports and environment setup"
      ],
      "metadata": {
        "id": "2LBDIKOrJmF9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# If use google colab, mount the working directory there. \n",
        "from google.colab import drive\n",
        "import sys\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# NOTE: you need to use your own path to add the implementation to the python path \n",
        "# so you can import functions from implementation.py\n",
        "sys.path.append('/content/drive/MyDrive/CS137_Assignment1_RobPitkin')"
      ],
      "metadata": {
        "id": "qAj_nR7oJbNj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# A bit of setup\n",
        "import numpy as np\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import sys\n",
        "\n",
        "%matplotlib inline\n",
        "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
        "plt.rcParams['image.interpolation'] = 'nearest'\n",
        "plt.rcParams['image.cmap'] = 'gray'\n",
        "\n",
        "# for auto-reloading external modules\n",
        "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
        "%load_ext autoreload\n",
        "%autoreload 2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2nICHUd8JP73",
        "outputId": "697b17a6-970e-4fbe-86e4-b925f876238b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The autoreload extension is already loaded. To reload it, use:\n",
            "  %reload_ext autoreload\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## UNet Helper Implementation"
      ],
      "metadata": {
        "id": "HfSRV_0AJpfw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Code derived from https://github.com/milesial/Pytorch-UNet/blob/master/unet/unet_parts.py"
      ],
      "metadata": {
        "id": "zdEkIluvZuRM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "CLOy1KouIvQU"
      },
      "outputs": [],
      "source": [
        "class ConvBlock(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    Convolutional block\n",
        "    \"\"\"\n",
        "    def __init__(self, num_filters=64, dropout_p=0):\n",
        "        super().__init__()\n",
        "\n",
        "        if dropout_p == 0:\n",
        "            self.conv_block = torch.nn.Sequential(\n",
        "                torch.nn.LazyConv2d(num_filters, kernel_size=3, padding='same'),\n",
        "                torch.nn.BatchNorm2d(num_filters),\n",
        "                torch.nn.ReLU(),\n",
        "                torch.nn.LazyConv2d(num_filters, kernel_size=3, padding='same'),\n",
        "                torch.nn.BatchNorm2d(num_filters),\n",
        "                torch.nn.ReLU()\n",
        "            )\n",
        "        else:\n",
        "            self.conv_block = torch.nn.Sequential(\n",
        "                torch.nn.LazyConv2d(num_filters, kernel_size=3, padding='same'),\n",
        "                torch.nn.BatchNorm2d(num_filters),\n",
        "                torch.nn.ReLU(),\n",
        "                torch.nn.LazyConv2d(num_filters, kernel_size=3, padding='same'),\n",
        "                torch.nn.BatchNorm2d(num_filters),\n",
        "                torch.nn.ReLU(),\n",
        "                torch.nn.Dropout(dropout_p)\n",
        "            )\n",
        "    \n",
        "    def forward(self, input):\n",
        "        return self.conv_block(input)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class DownBlock(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    Downsampling Block\n",
        "    \"\"\"\n",
        "    def __init__(self, out_channels, dropout_p=0):\n",
        "        super().__init__()\n",
        "\n",
        "        self.down_block = torch.nn.Sequential(\n",
        "            ConvBlock(out_channels, dropout_p),\n",
        "            torch.nn.MaxPool2d(kernel_size=2)\n",
        "        )\n",
        "    \n",
        "    def forward(self, input):\n",
        "        return self.down_block(input)"
      ],
      "metadata": {
        "id": "DLyig0tPMhgm"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class UpBlock(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    Upsampling Block\n",
        "    \"\"\"\n",
        "    def __init__(self, out_channels, dropout_p=0):\n",
        "        super().__init__()\n",
        "\n",
        "        self.up = torch.nn.LazyConvTranspose2d(out_channels, kernel_size=2, stride=2)\n",
        "        self.conv_block = ConvBlock(out_channels, dropout_p=dropout_p)\n",
        "    \n",
        "    def forward(self, input, skip):\n",
        "        \"\"\"\n",
        "        Using code derived from https://github.com/milesial/Pytorch-UNet/blob/master/unet/unet_parts.py\n",
        "        \"\"\"\n",
        "        input = self.up(input)\n",
        "        # input is CHW\n",
        "        diffY = skip.size()[2] - input.size()[2]\n",
        "        diffX = skip.size()[3] - input.size()[3]\n",
        "\n",
        "        input = torch.nn.functional.pad(input, [diffX // 2, diffX - diffX // 2,\n",
        "                                                diffY // 2, diffY - diffY // 2])\n",
        "       \n",
        "        x = torch.cat([skip, input], dim=1)\n",
        "        return self.conv_block(x)"
      ],
      "metadata": {
        "id": "vzVGuk5KNnVR"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class OutBlock(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    Final output block\n",
        "    \"\"\"\n",
        "    def __init__(self, out_channels):\n",
        "        super(OutBlock, self).__init__()\n",
        "\n",
        "        self.conv_layer = torch.nn.LazyConv2d(out_channels=out_channels, kernel_size=1, padding='same')\n",
        "    \n",
        "    def forward(self, input):\n",
        "        return self.conv_layer(input)\n"
      ],
      "metadata": {
        "id": "-13IHiHPayEF"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementing the UNet Model"
      ],
      "metadata": {
        "id": "-xetiYLgcK7_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class UNetModel(torch.nn.Module):\n",
        "    def __init__(self, num_classes=2, ):\n",
        "        super(UNetModel, self).__init__()\n",
        "\n",
        "        self.down_block1 = DownBlock(64)\n",
        "        self.down_block2 = DownBlock(128)\n",
        "        self.down_block3 = DownBlock(256)\n",
        "        self.down_block4 = DownBlock(512)\n",
        "        self.block5 = ConvBlock(1024)\n",
        "        self.up_block4 = UpBlock(512)\n",
        "        self.up_block3 = UpBlock(256)\n",
        "        self.up_block2 = UpBlock(128)\n",
        "        self.up_block1 = UpBlock(64)\n",
        "        self.out_block = OutBlock(num_classes)\n",
        "    \n",
        "    def forward(self, input):\n",
        "        x1 = self.down_block1(input)\n",
        "        x2 = self.down_block2(x1)\n",
        "        x3 = self.down_block3(x2)\n",
        "        x4 = self.down_block4(x3)\n",
        "        out = self.block5(x4)\n",
        "        out = self.up_block4(out, x4)\n",
        "        out = self.up_block3(out, x3)\n",
        "        out = self.up_block2(out, x2)\n",
        "        out = self.up_block1(out, x1)\n",
        "        out = self.out_block(out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "GBcoASBUcKKA"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Instantiating model and checking summary"
      ],
      "metadata": {
        "id": "dfFaXSAqejAh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchsummary import summary\n",
        "model = UNetModel()\n",
        "summary(model, input_size=(1, 512, 512))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Vyu863seaNz",
        "outputId": "40ab1cfc-fd9f-4a04-a7fb-fe60acee3eb3"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1         [-1, 64, 512, 512]             640\n",
            "       BatchNorm2d-2         [-1, 64, 512, 512]             128\n",
            "              ReLU-3         [-1, 64, 512, 512]               0\n",
            "            Conv2d-4         [-1, 64, 512, 512]          36,928\n",
            "       BatchNorm2d-5         [-1, 64, 512, 512]             128\n",
            "              ReLU-6         [-1, 64, 512, 512]               0\n",
            "         ConvBlock-7         [-1, 64, 512, 512]               0\n",
            "         MaxPool2d-8         [-1, 64, 256, 256]               0\n",
            "         DownBlock-9         [-1, 64, 256, 256]               0\n",
            "           Conv2d-10        [-1, 128, 256, 256]          73,856\n",
            "      BatchNorm2d-11        [-1, 128, 256, 256]             256\n",
            "             ReLU-12        [-1, 128, 256, 256]               0\n",
            "           Conv2d-13        [-1, 128, 256, 256]         147,584\n",
            "      BatchNorm2d-14        [-1, 128, 256, 256]             256\n",
            "             ReLU-15        [-1, 128, 256, 256]               0\n",
            "        ConvBlock-16        [-1, 128, 256, 256]               0\n",
            "        MaxPool2d-17        [-1, 128, 128, 128]               0\n",
            "        DownBlock-18        [-1, 128, 128, 128]               0\n",
            "           Conv2d-19        [-1, 256, 128, 128]         295,168\n",
            "      BatchNorm2d-20        [-1, 256, 128, 128]             512\n",
            "             ReLU-21        [-1, 256, 128, 128]               0\n",
            "           Conv2d-22        [-1, 256, 128, 128]         590,080\n",
            "      BatchNorm2d-23        [-1, 256, 128, 128]             512\n",
            "             ReLU-24        [-1, 256, 128, 128]               0\n",
            "        ConvBlock-25        [-1, 256, 128, 128]               0\n",
            "        MaxPool2d-26          [-1, 256, 64, 64]               0\n",
            "        DownBlock-27          [-1, 256, 64, 64]               0\n",
            "           Conv2d-28          [-1, 512, 64, 64]       1,180,160\n",
            "      BatchNorm2d-29          [-1, 512, 64, 64]           1,024\n",
            "             ReLU-30          [-1, 512, 64, 64]               0\n",
            "           Conv2d-31          [-1, 512, 64, 64]       2,359,808\n",
            "      BatchNorm2d-32          [-1, 512, 64, 64]           1,024\n",
            "             ReLU-33          [-1, 512, 64, 64]               0\n",
            "        ConvBlock-34          [-1, 512, 64, 64]               0\n",
            "        MaxPool2d-35          [-1, 512, 32, 32]               0\n",
            "        DownBlock-36          [-1, 512, 32, 32]               0\n",
            "           Conv2d-37         [-1, 1024, 32, 32]       4,719,616\n",
            "      BatchNorm2d-38         [-1, 1024, 32, 32]           2,048\n",
            "             ReLU-39         [-1, 1024, 32, 32]               0\n",
            "           Conv2d-40         [-1, 1024, 32, 32]       9,438,208\n",
            "      BatchNorm2d-41         [-1, 1024, 32, 32]           2,048\n",
            "             ReLU-42         [-1, 1024, 32, 32]               0\n",
            "        ConvBlock-43         [-1, 1024, 32, 32]               0\n",
            "  ConvTranspose2d-44          [-1, 512, 64, 64]       2,097,664\n",
            "           Conv2d-45          [-1, 512, 32, 32]       4,719,104\n",
            "      BatchNorm2d-46          [-1, 512, 32, 32]           1,024\n",
            "             ReLU-47          [-1, 512, 32, 32]               0\n",
            "           Conv2d-48          [-1, 512, 32, 32]       2,359,808\n",
            "      BatchNorm2d-49          [-1, 512, 32, 32]           1,024\n",
            "             ReLU-50          [-1, 512, 32, 32]               0\n",
            "        ConvBlock-51          [-1, 512, 32, 32]               0\n",
            "          UpBlock-52          [-1, 512, 32, 32]               0\n",
            "  ConvTranspose2d-53          [-1, 256, 64, 64]         524,544\n",
            "           Conv2d-54          [-1, 256, 64, 64]       1,179,904\n",
            "      BatchNorm2d-55          [-1, 256, 64, 64]             512\n",
            "             ReLU-56          [-1, 256, 64, 64]               0\n",
            "           Conv2d-57          [-1, 256, 64, 64]         590,080\n",
            "      BatchNorm2d-58          [-1, 256, 64, 64]             512\n",
            "             ReLU-59          [-1, 256, 64, 64]               0\n",
            "        ConvBlock-60          [-1, 256, 64, 64]               0\n",
            "          UpBlock-61          [-1, 256, 64, 64]               0\n",
            "  ConvTranspose2d-62        [-1, 128, 128, 128]         131,200\n",
            "           Conv2d-63        [-1, 128, 128, 128]         295,040\n",
            "      BatchNorm2d-64        [-1, 128, 128, 128]             256\n",
            "             ReLU-65        [-1, 128, 128, 128]               0\n",
            "           Conv2d-66        [-1, 128, 128, 128]         147,584\n",
            "      BatchNorm2d-67        [-1, 128, 128, 128]             256\n",
            "             ReLU-68        [-1, 128, 128, 128]               0\n",
            "        ConvBlock-69        [-1, 128, 128, 128]               0\n",
            "          UpBlock-70        [-1, 128, 128, 128]               0\n",
            "  ConvTranspose2d-71         [-1, 64, 256, 256]          32,832\n",
            "           Conv2d-72         [-1, 64, 256, 256]          73,792\n",
            "      BatchNorm2d-73         [-1, 64, 256, 256]             128\n",
            "             ReLU-74         [-1, 64, 256, 256]               0\n",
            "           Conv2d-75         [-1, 64, 256, 256]          36,928\n",
            "      BatchNorm2d-76         [-1, 64, 256, 256]             128\n",
            "             ReLU-77         [-1, 64, 256, 256]               0\n",
            "        ConvBlock-78         [-1, 64, 256, 256]               0\n",
            "          UpBlock-79         [-1, 64, 256, 256]               0\n",
            "           Conv2d-80          [-1, 2, 256, 256]             130\n",
            "         OutBlock-81          [-1, 2, 256, 256]               0\n",
            "================================================================\n",
            "Total params: 31,042,434\n",
            "Trainable params: 31,042,434\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 1.00\n",
            "Forward/backward pass size (MB): 2410.00\n",
            "Params size (MB): 118.42\n",
            "Estimated Total Size (MB): 2529.42\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VxeVkjAsf-PF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}